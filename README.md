# ðŸ¤– Transformers para GeraÃ§Ã£o de Texto com GPT-2

Este projeto demonstra o uso de modelos Transformers (especificamente o **GPT-2**) para geraÃ§Ã£o de textos contextualizados. Implementado no Google Colab, explora componentes-chave como tokenizaÃ§Ã£o, positional encoding, self-attention e geraÃ§Ã£o de sequÃªncias, utilizando a biblioteca Hugging Face Transformers.

---

## ðŸŽ¯ Objetivo

Treinar um pipeline de geraÃ§Ã£o de texto utilizando o GPT-2, compreendendo:
- PreparaÃ§Ã£o de texto via tokenizaÃ§Ã£o
- CodificaÃ§Ã£o posicional para contexto sequencial
- Mecanismos de self-attention
- GeraÃ§Ã£o de textos coerentes em diferentes contextos

---

## âš™ï¸ Tecnologias Utilizadas

- Python 3.10+
- Transformers 4.30+
- PyTorch / TensorFlow (via Hugging Face)
- Google Colab (com suporte opcional a GPU/TPU)
- Bibliotecas auxiliares: `bertviz`, `matplotlib`, `numpy`

---

## ðŸ”‘ Componentes Principais

### 1. **TokenizaÃ§Ã£o com BERT**
- Convertemos texto em tokens numÃ©ricos usando `BertTokenizer`.
- Exemplo: `"I read a good novel."` â†’ `['[CLS]', 'i', 'read', 'a', 'good', 'novel', '.', '[SEP]']`

### 2. **Positional Encoding**
- CodificaÃ§Ã£o senoidal para representar posiÃ§Ãµes de tokens.
- VisualizaÃ§Ã£o matricial das relaÃ§Ãµes posicionais.

### 3. **Self-Attention**
- VisualizaÃ§Ã£o interativa com `bertviz` para interpretar como o modelo relaciona palavras (ex: ambiguidade de "novel").

### 4. **GPT-2 para GeraÃ§Ã£o**
- GeraÃ§Ã£o de textos com o modelo prÃ©-treinado GPT-2 (137M parÃ¢metros).
- CustomizaÃ§Ã£o de parÃ¢metros como `max_length` e `num_return_sequences`.

---

## ðŸ› ï¸ ConfiguraÃ§Ã£o RÃ¡pida

```python
!pip install transformers matplotlib bertviz numpy

from transformers import pipeline
generator = pipeline('text-generation', model='gpt2')
generator("AI will change the future because", max_length=50, num_return_sequences=3)
```

---

## ðŸ“Š Exemplos de SaÃ­da

**Input**: `"Star Trek"`  
**Output** (truncado):
```text
[{'generated_text': 'Star Trek: Deep Space Nine...'},
 {'generated_text': 'Star Trek: Voyager - The Voyage Home...'},
 {'generated_text': 'Star Trek: The Next Generation episode...'}]
```

**Input**: `"This movie seemed really long."`  
**Output**: Gera crÃ­ticas cinematogrÃ¡ficas contextualizadas com atÃ© 300 tokens.

---

# âš ï¸ LimitaÃ§Ãµes Computacionais

A execuÃ§Ã£o do **GPT-2 Large** para geraÃ§Ã£o de textos apresenta os seguintes desafios tÃ©cnicos:

## ðŸ”§ **Requisitos de Hardware**
- **GPUs Dedicadas Recomendadas**:  
  - Tesla T4 / V100 (Google Colab Pro)  
  - NVIDIA A100 (AWS/GCP)  
  - *CPU nÃ£o Ã© recomendada*: Processamento extremamente lento (~10x mais demorado).

## ðŸš¨ **Problemas Comuns**
- **MemÃ³ria Insuficiente**:  
  O modelo consome > 5GB de VRAM (risco de `CUDA Out of Memory` em GPUs bÃ¡sicas).  
- **LatÃªncia Elevada**:  
  GeraÃ§Ãµes longas (`max_length > 100`) podem levar minutos sem GPU.  
- **Compatibilidade**:  
  VersÃµes antigas do PyTorch/Transformers podem causar conflitos.

---

**Nota**: O cÃ³digo para GPT-2 Large estÃ¡ comentado por padrÃ£o. Se tiver recursos adequados, descomente-o para testar o modelo completo.  

## ðŸ§  Conceitos Abordados

- Arquitetura Transformer
- TokenizaÃ§Ã£o e Embeddings Contextuais
- Mecanismos de AtenÃ§Ã£o Multi-Head
- GeraÃ§Ã£o Autoregressiva
- Interpretabilidade de Modelos via VisualizaÃ§Ã£o

---

## ðŸŒ AplicaÃ§Ãµes PrÃ¡ticas

- Geradores de conteÃºdo para marketing
- AssistÃªncia Ã  escrita criativa
- Chatbots contextualizados
- ExpansÃ£o de datasets para NLP
- Pesquisa em modelos de linguagem

---

## âœ… ConclusÃ£o

Este projeto ilustra o poder do GPT-2 para geraÃ§Ã£o de texto, desde a preparaÃ§Ã£o de dados atÃ© a produÃ§Ã£o de sequÃªncias complexas. A integraÃ§Ã£o de visualizaÃ§Ãµes (`bertviz`) e tÃ©cnicas como positional encoding oferece insights valiosos sobre o funcionamento interno de modelos Transformers.

---

- ðŸ“Œ **Autor**: Hian Stafford
- ðŸ“© **Email**: hian.correa@gmail.com
```text
  /\_/\  
 ( -.- ) 
  > ^ < 
```
